{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec1d06b6",
   "metadata": {},
   "source": [
    "# **PROJECT 4**\n",
    "\n",
    "# ðŸ”¥ **Detecting Fake News Using Machine Learning ðŸš€**\n",
    "\n",
    "### Fake news is a growing problem in today's society, and it can have serious consequences, including influencing public opinion and even shaping policy decisions. In this project, we aim to build a machine learning model that can accurately identify whether a given news article is fake or not.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "feca5946",
   "metadata": {
    "id": "4d5dda4c"
   },
   "source": [
    "Dataset used - https://www.kaggle.com/fake-news/data\n",
    "\n",
    "Dataset Description\n",
    "\n",
    "train.csv: A full training dataset with the following attributes:\n",
    "\n",
    "* id: unique id for a news article\n",
    "* title: the title of a news article\n",
    "* author: author of the news article\n",
    "* text: the text of the article; could be incomplete\n",
    "* label: a label that marks the article as potentially unreliable\n",
    "  * 1: FAKE\n",
    "  * 0: TRUE\n",
    "\n",
    "train.csv: The full training dataset, with the labels removed\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc05c85b",
   "metadata": {
    "id": "acdf7db5"
   },
   "source": [
    "### **Set the dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f313070",
   "metadata": {
    "id": "7b4b4979"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from api_keys import postgres_p\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "import pickle\n",
    "import winsound"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b920c2e",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9612db7e",
   "metadata": {
    "id": "f47d9fde"
   },
   "source": [
    "### **Retrieving Text Data from PostgreSQL and Loading into a Pandas DataFrame**\n",
    "\n",
    "First, the code downloads and loads all stop words from the NLTK corpus. Then, it establishes a connection to a PostgreSQL database named \"Project_4\" using the psycopg2 library. A SQL query is executed to retrieve data from two tables named \"article_id\" and \"text_corpus\" in the database. The data is limited to the first 1000 rows using the LIMIT keyword. The results are stored in a Pandas DataFrame named \"news_dataset\". Finally, the database connection is closed using the close() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c392dc4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b3d25c8",
    "outputId": "a174a4ae-f59d-4210-e4b1-32f29afa4c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download and load all stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc9173f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd8da98b",
    "outputId": "2a30bc0b-3d38-431f-dc5a-1be22dfd8839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# check if Stopwords loaded in english\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5becb953",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67432b17",
   "metadata": {
    "id": "a44f0a84"
   },
   "source": [
    "### **Data Pre-processing and Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ea1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(database=\"Project_4\", user=\"postgres\", password=postgres_p) #host=\"your_host_address\", port=\"your_port_number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b838505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to retrieve the data - Limit to 1000 records for testing and evaluation\n",
    "query = \"SELECT a.article_id, a.article_label, t.text_corpus FROM article_id a  JOIN text_corpus t ON a.article_id = t.article_id LIMIT 5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd6955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a Pandas DataFrame\n",
    "news_dataset = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1ffc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd99892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Darrell Lucus House Dem Aide: We Didnâ€™t Even S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Consortiumnews.com Why the Truth Might Get You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Jessica Purkiss 15 Civilians Killed In Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Howard Portnoy Iranian woman jailed for fictio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  article_label  \\\n",
       "0           0              1   \n",
       "1           1              0   \n",
       "2           2              1   \n",
       "3           3              1   \n",
       "4           4              1   \n",
       "\n",
       "                                         text_corpus  \n",
       "0  Darrell Lucus House Dem Aide: We Didnâ€™t Even S...  \n",
       "1  Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
       "2  Consortiumnews.com Why the Truth Might Get You...  \n",
       "3  Jessica Purkiss 15 Civilians Killed In Single ...  \n",
       "4  Howard Portnoy Iranian woman jailed for fictio...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset\n",
    "news_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0ce74f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e3f4e1",
   "metadata": {},
   "source": [
    "### **Text Preprocessing with Stemming, Stop Word Removal, and Lowercasing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50cdbb17",
   "metadata": {},
   "source": [
    "In the following code, the data and label are separated into two variables, X and Y, respectively, from the previously loaded dataset. Then, a function is defined to perform stemming on the text data. \n",
    "\n",
    "Stemming is the process of reducing words to their base or root form. The PorterStemmer class from the nltk package is used for stemming. \n",
    "\n",
    "The function removes non-alphabetic characters and converts all remaining letters to lowercase. It splits the text into individual words, removes stop words, and applies stemming to each word using the PorterStemmer. Finally, it joins all the words back into a string. \n",
    "\n",
    "The apply() method is used to apply the stemming() function to each row of the text_corpus column of the dataset, and the results are stored back in X. \n",
    "\n",
    "Lastly, a beep sound is played to signal that the process has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d214d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data and label into two variables\n",
    "X = news_dataset['text_corpus']\n",
    "Y = news_dataset['article_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d98a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stemming the content\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(content):\n",
    "    # Pick all alphabet characters - lowercase and uppercase...all others such as numbers and punctuations will be removed. Numbers or punctuations will be replaced by a whitespace\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    # Converting all letters to lowercase \n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    # Converting all to a splitted case or a list\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    # Applying stemming, so we get the root words wherever possible + remove stopwords as well\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    # Join all the words in final content\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6c0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the text_corpus column\n",
    "X = X.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a sound to let you know its done \n",
    "duration = 2000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24529202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the X and Y variables\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c81f2433",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5583b7ea",
   "metadata": {},
   "source": [
    "### **Transforming Text Data with CountVectorizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b52fc171",
   "metadata": {},
   "source": [
    "In the following code, a single instance of CountVectorizer is created to convert text data into numerical form. The stop_words parameter is set to 'english' to remove common stop words from the text. The fit() method is called on the vectorizer object to learn the vocabulary of the text data. The transform() method is called on the X variable to convert the text data into a sparse matrix of numerical features. The resulting matrix is stored in X_transformed.\n",
    "\n",
    "The pickle module is used to save the vectorizer object to a file named \"tfidfvect2.pkl\" in the \"Pickles\" directory. The saved object can be loaded and used later for transforming new text data into numerical features.\n",
    "\n",
    "The print() function is used to display the vectorizer object and the transformed X data. The transformed X data is displayed as a sparse matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c36b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d004944",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "0543b31e",
    "outputId": "f81bb5a9-d380-4a84-9d7a-6e3c892a42bc"
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer on the text data\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into numerical features\n",
    "X_transformed = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5a306",
   "metadata": {
    "id": "YZO9QQZoD8_r"
   },
   "outputs": [],
   "source": [
    "# Save the vectorizer object to a file using pickle\n",
    "pickle.dump(vectorizer, open('../Pickles/tfidfvect2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved object and print it to check if it is loaded correctly\n",
    "TEST_model = pickle.load(open('../Pickles/tfidfvect2.pkl', 'rb'))\n",
    "print(TEST_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7806bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95438d19",
    "outputId": "190f59a3-8ed5-481e-8ad0-de3cd3baaadd"
   },
   "outputs": [],
   "source": [
    "# Print the transformed data\n",
    "print(X_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56089711",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b35a4a4",
   "metadata": {
    "id": "0f84ef5f"
   },
   "source": [
    "### **CLASSIFICATION MODEL : LOGISTIC REGRESSION CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b30fa",
   "metadata": {
    "id": "7a246d29"
   },
   "source": [
    "### Splitting the data into test and train datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98f8c3ba",
   "metadata": {},
   "source": [
    "The data is split into training and testing sets using the train_test_split function. The test_size parameter is set to 0.18, meaning that 18% of the data will be used for testing, and the remaining 82% for training. The random_state parameter is set to 42 to ensure reproducibility of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46151891",
   "metadata": {
    "id": "6007c5c2"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into test and train datasets\n",
    "X_train_transformed, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.18, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "804762c0",
   "metadata": {
    "id": "b653caf9"
   },
   "source": [
    "A parameter grid is defined for hyperparameter tuning, including regularization strength C, regularization penalty type (l1 or l2), and solver algorithm (liblinear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Logistic Regression\n",
    "logreg_param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab822b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26c11f4",
   "metadata": {},
   "source": [
    "GridSearchCV is used to find the best set of hyperparameters from the parameter grid. Cross-Validation with 5-folds is used to evaluate each set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Logistic Regression\n",
    "logreg_grid_search = GridSearchCV(logreg, logreg_param_grid, cv=5, verbose=0)\n",
    "logreg_grid_search.fit(X_transformed, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "810552b2",
   "metadata": {},
   "source": [
    "The best hyperparameters found by Grid Search are stored in best_logreg_params and are then used to train the final Logistic Regression model, best_logreg_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed31e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best parameters\n",
    "best_logreg_params = logreg_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc65543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best parameters\n",
    "best_logreg_model = LogisticRegression(**best_logreg_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19cfeddd",
   "metadata": {},
   "source": [
    "The trained model is used to predict the outcomes on the test set, and the prediction accuracy is calculated as the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28220e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracies and record the changes\n",
    "logreg_accuracies = logreg_grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_model.fit(X_train_transformed, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cf69359",
   "metadata": {},
   "source": [
    "The results are displayed, including the accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = best_logreg_model.predict(X_test)\n",
    "accuracy1 = np.mean(y_pred1 == Y_test) * 100\n",
    "\n",
    "print(\"Logistic Regression Model Results\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy1))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(classification_report(Y_test, y_pred1))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred1))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7a66abf",
   "metadata": {},
   "source": [
    "A plot is created to visualize the accuracy of different sets of hyperparameters during the Grid Search process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(logreg_accuracies, label=\"Logistic Regression\", linestyle=\"-\", marker=\"o\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"logreg_model_optimization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "825c7a33",
   "metadata": {},
   "source": [
    "The trained model is used to predict the outcomes on the training and test sets, and the prediction accuracy is calculated using the accuracy_score function from the sklearn.metrics module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98872c4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "304118ab",
    "outputId": "99ce9aec-a7b9-41f4-eefb-bf84c5d7a992"
   },
   "outputs": [],
   "source": [
    "# Accuracy Score on Training Data\n",
    "X_train_prediction = best_logreg_model.predict(X_train_transformed)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "\n",
    "print('Accuracy score on the training data: ',training_data_accuracy)\n",
    "\n",
    "# Accuracy Score on Test Data\n",
    "X_test_prediction = best_logreg_model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
    "\n",
    "print('Accuracy score on the test data: ',test_data_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acfee21e",
   "metadata": {},
   "source": [
    "The trained Logistic Regression model is saved as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f85195",
   "metadata": {
    "id": "e2b32a09"
   },
   "outputs": [],
   "source": [
    "pickle.dump(best_logreg_model, open('../Pickles/logisticreg_model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d31885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c34ea110",
   "metadata": {
    "id": "b26ebf70"
   },
   "source": [
    "### **CLASSIFICATION MODEL : PASSIVE AGGRESSIVE CLASSIFIER**\n",
    "\n",
    "Passive Aggressive Classifier works by responding as passive for correct classifications and responding as aggressive for any miscalculation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccd318bc",
   "metadata": {},
   "source": [
    "A function called stemming is defined to preprocess and stem the input text. The function takes the input text, removes non-alphabetic characters, converts the text to lowercase, tokenizes it, removes stopwords, and applies stemming using the PorterStemmer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62852b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess and stem the text\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    return review\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9536f915",
   "metadata": {},
   "source": [
    "The stemming function is applied to each element of X. The preprocessed data is then transformed using the vectorizer, and the dataset is split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba83559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to each text in the array\n",
    "X_preprocessed = [stemming(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed data\n",
    "X_transformed = vectorizer.fit_transform(X_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde46919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf0da49",
   "metadata": {},
   "source": [
    "A parameter grid is defined for hyperparameter tuning, including regularization strength C, and loss function type (hinge or squared_hinge). GridSearchCV is used to find the best set of hyperparameters from the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d45886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Passive Aggressive Classifier\n",
    "pac_param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf722d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "pac = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Passive Aggressive Classifier\n",
    "pac_grid_search = GridSearchCV(pac, pac_param_grid, cv=5, verbose=0)\n",
    "pac_grid_search.fit(X_transformed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best parameters\n",
    "best_pac_params = pac_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50023b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best parameters\n",
    "best_pac_model = PassiveAggressiveClassifier(**best_pac_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracies and record the changes\n",
    "pac_accuracies = pac_grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pac_model.fit(X_train, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9896492",
   "metadata": {},
   "source": [
    "The results are displayed, including the accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "674340b7",
   "metadata": {},
   "source": [
    "A plot is created to visualize the accuracy of different sets of hyperparameters during the Grid Search process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pac_accuracies, label=\"Passive Aggressive Classifier\", linestyle=\"--\", marker=\"x\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Passive Aggressive Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_optimization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive Aggressive Classifier\n",
    "y_pred2 = best_pac_model.predict(X_test)\n",
    "accuracy2 = np.mean(y_pred2 == Y_test) * 100\n",
    "\n",
    "print(\"Passive Aggressive Classifier Model Results\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(classification_report(Y_test, y_pred2))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction on test set\n",
    "test_pred = best_pac_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5973cd",
   "metadata": {},
   "source": [
    "The trained TfidfVectorizer instance and the Passive Aggressive Classifier model are saved as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "pickle.dump(vectorizer, open('../Pickles/tfidf_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9170565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(best_pac_model, open('../Pickles/passive_aggressive_model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf34f3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4217aa3c",
   "metadata": {
    "id": "0eb9e924"
   },
   "source": [
    "## Testing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and vectorize the input data\n",
    "X_preprocessed = [stemming(text) for text in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test sets\n",
    "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(X_preprocessed, Y, test_size=0.18, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ebd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer on the training data and transform both the training and test data\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Logistic Regression model with the best parameters\n",
    "best_logreg_model = LogisticRegression(**best_logreg_params)\n",
    "best_logreg_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0aa87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Logistic Regression Model\n",
    "y_pred = best_logreg_model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == Y_test) * 100\n",
    "print(\"Logistic Regression Model Prediction Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Passive Aggressive Classifier model with the best parameters\n",
    "best_pac_model = PassiveAggressiveClassifier(**best_pac_params)\n",
    "best_pac_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49237bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Passive Aggressive Classifier Model\n",
    "y2_pred = best_pac_model.predict(X_test)\n",
    "accuracy = np.mean(y2_pred == Y_test) * 100\n",
    "print(\"Passive Aggressive Classifier Model Prediction Accuracy: {:.2f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(logreg_accuracies, label=\"Logistic Regression\", linestyle=\"-\", marker=\"o\")\n",
    "plt.plot(pac_accuracies, label=\"Passive Aggressive Classifier\", linestyle=\"--\", marker=\"x\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_optimization.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2140.826204,
   "end_time": "2021-09-29T17:10:10.602486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T16:34:29.776282",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
