{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24755160",
   "metadata": {
    "id": "8c9cb573"
   },
   "source": [
    "This code reads in two CSV files named 'test.csv' and 'train.csv', cleans the data by filling in missing values with empty strings, creates a new column in the training dataframe called 'text_corpus' by concatenating the 'author', 'title', and 'text' columns, and then creates several dataframes for different columns in the training dataframe. It also creates a date table for article dates and loads all the dataframes into a PostgreSQL database using SQLAlchemy.\n",
    "\n",
    "After loading the data into the database, it uses pd.read_sql_query() to query the database and display the first 15 rows of each table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca5946",
   "metadata": {
    "id": "4d5dda4c"
   },
   "source": [
    "#### Dataset used - https://www.kaggle.com/fake-news/data\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "train.csv: A full training dataset with the following attributes:\n",
    "\n",
    "* id: unique id for a news article\n",
    "* title: the title of a news article\n",
    "* author: author of the news article\n",
    "* text: the text of the article; could be incomplete\n",
    "* label: a label that marks the article as potentially unreliable\n",
    "  * 1: FAKE\n",
    "  * 0: TRUE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9612db7e",
   "metadata": {
    "id": "f47d9fde"
   },
   "source": [
    "First, the code downloads and loads all stop words from the NLTK corpus. Then, it establishes a connection to a PostgreSQL database named \"Project_4\" using the psycopg2 library. A SQL query is executed to retrieve data from two tables named \"article_id\" and \"text_corpus\" in the database. The data is limited to the first 1000 rows using the LIMIT keyword. The results are stored in a Pandas DataFrame named \"news_dataset\". Finally, the database connection is closed using the close() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05c85b",
   "metadata": {
    "id": "acdf7db5"
   },
   "source": [
    "Set the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6f313070",
   "metadata": {
    "id": "7b4b4979"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from api_keys import postgres_p\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "import pickle\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19d1d8",
   "metadata": {
    "id": "5cbd1c06"
   },
   "source": [
    "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
    "The nltk.corpus package defines a collection of corpus reader classes, which can be used to access the contents of a diverse set of corpora. The list of available corpora is given at: https://www.nltk.org/nltk_data/ Each corpus reader class is specialized to handle a specific corpus forma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94e780",
   "metadata": {
    "id": "c32358ea"
   },
   "source": [
    "Load and test all the STW ,Stopwords are words which occur frequently in a corpus. e.g a, an, the, in. Frequently occurring words are removed from the corpus for the purpose of text-normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c392dc4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b3d25c8",
    "outputId": "a174a4ae-f59d-4210-e4b1-32f29afa4c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download and load all stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "efc9173f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd8da98b",
    "outputId": "2a30bc0b-3d38-431f-dc5a-1be22dfd8839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# check if Stopwords loaded in english\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67432b17",
   "metadata": {
    "id": "a44f0a84"
   },
   "source": [
    "Data Pre-processing and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cb37a",
   "metadata": {
    "id": "83fffc5d"
   },
   "source": [
    "Regular Expression Syntax. A regular expression (or RE) specifies a set of strings that matches it; the functions in this module let you check if a particular string matches a given regular expression (or if a given regular expression matches a particular string, which comes down to the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d1ea1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(database=\"Project_4\", user=\"postgres\", password=postgres_p) #host=\"your_host_address\", port=\"your_port_number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3b838505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to retrieve the data - Limit to 1000 records for testing and evaluation\n",
    "query = \"SELECT a.article_id, a.article_label, t.text_corpus FROM article_id a  JOIN text_corpus t ON a.article_id = t.article_id LIMIT 1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "edd6955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a Pandas DataFrame\n",
    "news_dataset = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ca1ffc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8fd99892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_label</th>\n",
       "      <th>text_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Darrell Lucus House Dem Aide: We Didn’t Even S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Consortiumnews.com Why the Truth Might Get You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Jessica Purkiss 15 Civilians Killed In Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Howard Portnoy Iranian woman jailed for fictio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  article_label  \\\n",
       "0           0              1   \n",
       "1           1              0   \n",
       "2           2              1   \n",
       "3           3              1   \n",
       "4           4              1   \n",
       "\n",
       "                                         text_corpus  \n",
       "0  Darrell Lucus House Dem Aide: We Didn’t Even S...  \n",
       "1  Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...  \n",
       "2  Consortiumnews.com Why the Truth Might Get You...  \n",
       "3  Jessica Purkiss 15 Civilians Killed In Single ...  \n",
       "4  Howard Portnoy Iranian woman jailed for fictio...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d214d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will separate the data and label i.e. text_corpus and label fields\n",
    "X = news_dataset['text_corpus']\n",
    "Y = news_dataset['article_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stemming the content\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(content):\n",
    "    # Pick all alphabet characters - lowercase and uppercase...all others such as numbers and punctuations will be removed. Numbers or punctuations will be replaced by a whitespace\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    # Converting all letters to lowercase \n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    # Converting all to a splitted case or a list\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    # Applying stemming, so we get the root words wherever possible + remove stopwords as well\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    # Join all the words in final content\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the text_corpus column\n",
    "X = X.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a sound to let you know its done \n",
    "duration = 2000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24529202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the X and Y variables\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c36b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d004944",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "0543b31e",
    "outputId": "f81bb5a9-d380-4a84-9d7a-6e3c892a42bc"
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5a306",
   "metadata": {
    "id": "YZO9QQZoD8_r"
   },
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('../Pickles/tfidfvect2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_model = pickle.load(open('../Pickles/tfidfvect2.pkl', 'rb'))\n",
    "\n",
    "print(TEST_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7806bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95438d19",
    "outputId": "190f59a3-8ed5-481e-8ad0-de3cd3baaadd"
   },
   "outputs": [],
   "source": [
    "print(X_transformed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56089711",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35a4a4",
   "metadata": {
    "id": "0f84ef5f"
   },
   "source": [
    "Modeling & Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b30fa",
   "metadata": {
    "id": "7a246d29"
   },
   "source": [
    "### Splitting the data into test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46151891",
   "metadata": {
    "id": "6007c5c2"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into test and train datasets\n",
    "X_train_transformed, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.18, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804762c0",
   "metadata": {
    "id": "b653caf9"
   },
   "source": [
    "We use 2 models to determine the accuracy of teh training set and will then select the most accurate model to us ein HEREKO\n",
    "The first Model - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d86a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "1915d04c",
    "outputId": "e7d71995-1315-4f1a-9f58-68e38565346e",
    "tags": [
     "history"
    ]
   },
   "outputs": [],
   "source": [
    "# # Training the model\n",
    "# logisticreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# logisticreg_model.fit(X_train_transformed, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Logistic Regression\n",
    "logreg_param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab822b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Logistic Regression\n",
    "logreg_grid_search = GridSearchCV(logreg, logreg_param_grid, cv=5, verbose=0)\n",
    "logreg_grid_search.fit(X_transformed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed31e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best parameters\n",
    "best_logreg_params = logreg_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc65543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best parameters\n",
    "best_logreg_model = LogisticRegression(**best_logreg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28220e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracies and record the changes\n",
    "logreg_accuracies = logreg_grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_model.fit(X_train_transformed, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 1: Logistic Regression\n",
    "y_pred1 = best_logreg_model.predict(X_test)\n",
    "accuracy1 = np.mean(y_pred1 == Y_test) * 100\n",
    "\n",
    "print(\"Logistic Regression Model Results\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy1))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(classification_report(Y_test, y_pred1))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred1))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(logreg_accuracies, label=\"Logistic Regression\", linestyle=\"-\", marker=\"o\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"logreg_model_optimization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e7c25",
   "metadata": {
    "id": "0a019bc4"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98872c4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "304118ab",
    "outputId": "99ce9aec-a7b9-41f4-eefb-bf84c5d7a992"
   },
   "outputs": [],
   "source": [
    "# Accuracy Score on Training Data\n",
    "X_train_prediction = best_logreg_model.predict(X_train_transformed)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
    "\n",
    "print('Accuracy score on the training data: ',training_data_accuracy)\n",
    "\n",
    "# Accuracy Score on Test Data\n",
    "X_test_prediction = best_logreg_model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
    "\n",
    "print('Accuracy score on the test data: ',test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f85195",
   "metadata": {
    "id": "e2b32a09"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(logisticreg_model, open('../Pickles/logisticreg_model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ded5a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "014c5ef3",
    "outputId": "82bed337-cd23-4c88-f9a5-e3acd47f23c3"
   },
   "outputs": [],
   "source": [
    "# Classification report for test data\n",
    "classification_report(Y_test, X_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ea110",
   "metadata": {
    "id": "b26ebf70"
   },
   "source": [
    "**CLASSIFICATION MODEL : PASSIVE AGGRESSIVE CLASSIFIER**\n",
    "\n",
    "* Passive Aggressive Classifier works by responding as passive for correct classifications and responding as aggressive for any miscalculation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d31885",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62852b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess and stem the text\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba83559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply stemming to each text in the array\n",
    "X_preprocessed = [stemming(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the preprocessed data\n",
    "X_transformed = vectorizer.fit_transform(X_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde46919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d45886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Passive Aggressive Classifier\n",
    "pac_param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf722d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "pac = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV for Passive Aggressive Classifier\n",
    "pac_grid_search = GridSearchCV(pac, pac_param_grid, cv=5, verbose=0)\n",
    "pac_grid_search.fit(X_transformed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best parameters\n",
    "best_pac_params = pac_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50023b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with best parameters\n",
    "best_pac_model = PassiveAggressiveClassifier(**best_pac_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracies and record the changes\n",
    "pac_accuracies = pac_grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pac_model.fit(X2_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Passive Aggressive Classifier\n",
    "y_pred2 = best_pac_model.predict(X2_test)\n",
    "accuracy2 = np.mean(y_pred2 == Y2_test) * 100\n",
    "\n",
    "print(\"Passive Aggressive Classifier Model Results\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy2))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(classification_report(Y2_test, y_pred2))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y2_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot changes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pac_accuracies, label=\"Passive Aggressive Classifier\", linestyle=\"--\", marker=\"x\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Passive Aggressive Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_optimization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction on test set\n",
    "test_pred = best_pac_model.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "pickle.dump(vectorizer, open('../Pickles/tfidf_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9170565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(best_pac_model, open('../Pickles/passive_aggressive_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained models to make predictions on new data\n",
    "vectorizer = pickle.load(open('../Pickles/tfidf_vectorizer.pkl', 'rb'))\n",
    "passive_aggressive_model = pickle.load(open('../Pickles/passive_aggressive_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_new is a new text input\n",
    "X_new = X[5]\n",
    "X_new_preprocessed = stemming(X_new)\n",
    "X_new_transformed = vectorizer.transform([X_new_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1336282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make prediction using the trained model\n",
    "prediction = passive_aggressive_model.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prediction result\n",
    "print(\"Prediction for the new text input: \", prediction[0])\n",
    "if (prediction[0] == 0):\n",
    "    print('Jono says it\\'s True')\n",
    "else:\n",
    "    print('Johan Says it is a porky:)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf34f3c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217aa3c",
   "metadata": {
    "id": "0eb9e924"
   },
   "source": [
    "Testing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the input data\n",
    "X_transformed = vectorizer.fit_transform(X)\n",
    "\n",
    "# Splitting dataset into train and test sets\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X_transformed, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Creating model\n",
    "passiveagressive_model = PassiveAggressiveClassifier(C=0.5, random_state=5)\n",
    "\n",
    "# Fitting model\n",
    "passiveagressive_model.fit(X2_train, Y2_train)\n",
    "\n",
    "# Making prediction on test set\n",
    "test_pred = passiveagressive_model.predict(X2_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(f\"Test Set Accuracy : {accuracy_score(Y2_test, test_pred) * 100} %\\n\\n\")\n",
    "\n",
    "# # Save the vectorizer\n",
    "# pickle.dump(vectorizer, open('../Pickles/tfidf_vectorizer.pkl', 'wb'))\n",
    "\n",
    "# # Save the model\n",
    "# pickle.dump(passiveagressive_model, open('../Pickles/passiveagressive_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bda029",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_logreg_model.predict(X2_test)\n",
    "\n",
    "# Calculate the prediction accuracy\n",
    "accuracy = np.mean(y_pred == Y_test) * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Print the prediction for a single example\n",
    "X_new = X2_test[5]\n",
    "prediction = best_logreg_model.predict(X_new.reshape(1, -1))\n",
    "print(\"Prediction for example 500: \", prediction[0])\n",
    "if (prediction[0] == 0):\n",
    "  print('Jono says its True')\n",
    "else:\n",
    "  print('Johan Says it is a porky:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee542d9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f35d01a",
    "outputId": "87003203-58d1-468f-febd-08cb3f9497a4"
   },
   "outputs": [],
   "source": [
    "y2_pred = best_pac_model.predict(X2_test)\n",
    "\n",
    "# Calculate the prediction accuracy\n",
    "accuracy = np.mean(y2_pred == Y2_test) * 100\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Prediction accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Print the prediction for a single example\n",
    "X2_new = X2_test[5]\n",
    "prediction2 = best_pac_model.predict(X2_new.reshape(1, -1))\n",
    "print(\"Prediction for example 500: \", prediction[0])\n",
    "if (prediction[0] == 0):\n",
    "  print('Jono says its True')\n",
    "else:\n",
    "  print('Johan Says it is a porky:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded013bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "53216c3b",
    "outputId": "b3ddee1f-8b8d-4c16-b4bd-05d12eabacd6"
   },
   "outputs": [],
   "source": [
    "news_dataset[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbeec6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e7f51b5",
    "outputId": "5c86dc3f-cc8f-45a5-961a-7172d3859859"
   },
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c753f79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c9261cf",
    "outputId": "54f1a90e-b597-4823-b1ac-95ccd39eb9e2"
   },
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81e358",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "0c6b5a46",
    "outputId": "66f30668-2fac-47d4-a137-34e61c44b8d2"
   },
   "outputs": [],
   "source": [
    "news_dataset[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839febd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is an array of text inputs\n",
    "X_preprocessed = [stemming(text) for text in X]  # Apply stemming to each text in the array\n",
    "X_vectorized = vectorizer.transform(X_preprocessed)  # Convert to numerical format using the trained vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71729407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96a37b98",
    "outputId": "c624e03a-b09b-4801-e8c6-7bd1ca6109a1"
   },
   "outputs": [],
   "source": [
    "pickled_model1 = pickle.load(open('../Pickles/logisticreg_model.pkl', 'rb'))\n",
    "pickled_model1.predict(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a95d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ab2357c",
    "outputId": "8151f4d9-d005-4d08-c4f7-fdace7e0da51"
   },
   "outputs": [],
   "source": [
    "pickled_model2 = pickle.load(open('../Pickles/passiveagressive_model.pkl', 'rb'))\n",
    "pickled_model2.predict(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e0161",
   "metadata": {
    "id": "170922ea"
   },
   "source": [
    "FAngo tested a point to clarify teh vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8ed22",
   "metadata": {
    "id": "b0287aa5"
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85837835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "8144960d",
    "outputId": "4b1c42c4-0e78-4936-a1df-b47753a34daa"
   },
   "outputs": [],
   "source": [
    "review = re.sub('[^a-zA-Z]', ' ', news_dataset['text_corpus'][10])\n",
    "review = review.lower()\n",
    "review = review.split()\n",
    "review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9936d",
   "metadata": {
    "id": "79886864"
   },
   "outputs": [],
   "source": [
    "val = vectorizer.transform([review]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b0a25",
   "metadata": {
    "id": "884b6a48"
   },
   "outputs": [],
   "source": [
    "tfidfvect2_model2 = pickle.load(open('../Pickles/tfidfvect2.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(logreg_accuracies, label=\"Logistic Regression\", linestyle=\"-\", marker=\"o\")\n",
    "plt.plot(pac_accuracies, label=\"Passive Aggressive Classifier\", linestyle=\"--\", marker=\"x\")\n",
    "plt.xlabel(\"Parameter Set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Optimization\")\n",
    "plt.legend()\n",
    "plt.savefig(\"model_optimization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c5161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e10c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2140.826204,
   "end_time": "2021-09-29T17:10:10.602486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-29T16:34:29.776282",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
